{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import get_data, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up experiment parameters\n",
    "params = utils.Params(\"model_configurations/experiment_params.json\")\n",
    "X, y, cols = get_data.get_and_preprocess_german(params)\n",
    "\n",
    "features = [c for c in X]\n",
    "\n",
    "X = X.values\n",
    "scaler = MinMaxScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "# Prepare data loader\n",
    "xtrain = torch.from_numpy(xtrain)\n",
    "ytrain = torch.from_numpy(ytrain)\n",
    "xtest = torch.from_numpy(xtest)\n",
    "ytest = torch.from_numpy(ytest)\n",
    "\n",
    "train_dataset = TensorDataset(xtrain, ytrain)\n",
    "test_dataset = TensorDataset(xtest, ytest)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # TODO neural net hyper-parameters\n",
    "        \"\"\"self.fc1 = nn.Linear(28, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 28)\"\"\"\n",
    "\n",
    "        self.fc1 = nn.Linear(28, 60)\n",
    "        self.fc21 = nn.Linear(60, 30)\n",
    "        self.fc22 = nn.Linear(60, 30)\n",
    "        self.fc3 = nn.Linear(30, 60)\n",
    "        self.fc4 = nn.Linear(60, 28)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 28))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "mu, logvar = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       loss.item() / len(data)))\n",
    "            #print(\"Mu: {} \\t logvar: {}\".format(mu, logvar))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.float().to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            \"\"\"\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "            \"\"\"\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for epoch in range(1, 30 + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "\n",
    "        if 1:\n",
    "            torch.save(model.state_dict(), \"vae_lime_testing.pt\")\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(5, 30).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "\n",
    "        # TODO Inverse transform not one-hot ?!\n",
    "        inversed = scaler.inverse_transform(sample)\n",
    "        np.set_printoptions(suppress=True)\n",
    "        #print(sample)\n",
    "        #print(inversed)\n",
    "\n",
    "        s = [np.round(i, 0) for i in inversed]\n",
    "        for a in s:\n",
    "            print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/900 (0%)]\tLoss: 21.089291\n",
      "====> Epoch: 1 Average loss: 20.5319\n",
      "====> Test set loss: 19.6274\n",
      "Train Epoch: 2 [0/900 (0%)]\tLoss: 19.757406\n",
      "====> Epoch: 2 Average loss: 19.3203\n",
      "====> Test set loss: 18.7017\n",
      "Train Epoch: 3 [0/900 (0%)]\tLoss: 18.783426\n",
      "====> Epoch: 3 Average loss: 18.4211\n",
      "====> Test set loss: 17.9346\n",
      "Train Epoch: 4 [0/900 (0%)]\tLoss: 17.935833\n",
      "====> Epoch: 4 Average loss: 17.5920\n",
      "====> Test set loss: 16.9849\n",
      "Train Epoch: 5 [0/900 (0%)]\tLoss: 17.184919\n",
      "====> Epoch: 5 Average loss: 16.8547\n",
      "====> Test set loss: 16.2775\n",
      "Train Epoch: 6 [0/900 (0%)]\tLoss: 16.570992\n",
      "====> Epoch: 6 Average loss: 16.1522\n",
      "====> Test set loss: 15.6970\n",
      "Train Epoch: 7 [0/900 (0%)]\tLoss: 15.700336\n",
      "====> Epoch: 7 Average loss: 15.4961\n",
      "====> Test set loss: 15.0703\n",
      "Train Epoch: 8 [0/900 (0%)]\tLoss: 15.265775\n",
      "====> Epoch: 8 Average loss: 14.9392\n",
      "====> Test set loss: 14.4157\n",
      "Train Epoch: 9 [0/900 (0%)]\tLoss: 14.616173\n",
      "====> Epoch: 9 Average loss: 14.4637\n",
      "====> Test set loss: 14.3351\n",
      "Train Epoch: 10 [0/900 (0%)]\tLoss: 14.262723\n",
      "====> Epoch: 10 Average loss: 14.1048\n",
      "====> Test set loss: 13.8601\n",
      "Train Epoch: 11 [0/900 (0%)]\tLoss: 13.672221\n",
      "====> Epoch: 11 Average loss: 13.8402\n",
      "====> Test set loss: 13.6444\n",
      "Train Epoch: 12 [0/900 (0%)]\tLoss: 13.435642\n",
      "====> Epoch: 12 Average loss: 13.5510\n",
      "====> Test set loss: 13.5355\n",
      "Train Epoch: 13 [0/900 (0%)]\tLoss: 13.618611\n",
      "====> Epoch: 13 Average loss: 13.4110\n",
      "====> Test set loss: 13.4113\n",
      "Train Epoch: 14 [0/900 (0%)]\tLoss: 13.575553\n",
      "====> Epoch: 14 Average loss: 13.2669\n",
      "====> Test set loss: 13.4853\n",
      "Train Epoch: 15 [0/900 (0%)]\tLoss: 13.302221\n",
      "====> Epoch: 15 Average loss: 13.1519\n",
      "====> Test set loss: 13.0841\n",
      "Train Epoch: 16 [0/900 (0%)]\tLoss: 12.940457\n",
      "====> Epoch: 16 Average loss: 13.0125\n",
      "====> Test set loss: 13.0528\n",
      "Train Epoch: 17 [0/900 (0%)]\tLoss: 12.780502\n",
      "====> Epoch: 17 Average loss: 12.9522\n",
      "====> Test set loss: 12.9482\n",
      "Train Epoch: 18 [0/900 (0%)]\tLoss: 12.920980\n",
      "====> Epoch: 18 Average loss: 12.9322\n",
      "====> Test set loss: 12.9167\n",
      "Train Epoch: 19 [0/900 (0%)]\tLoss: 13.019342\n",
      "====> Epoch: 19 Average loss: 12.8677\n",
      "====> Test set loss: 13.0171\n",
      "Train Epoch: 20 [0/900 (0%)]\tLoss: 12.780175\n",
      "====> Epoch: 20 Average loss: 12.7759\n",
      "====> Test set loss: 12.8176\n",
      "Train Epoch: 21 [0/900 (0%)]\tLoss: 12.702296\n",
      "====> Epoch: 21 Average loss: 12.7371\n",
      "====> Test set loss: 12.8899\n",
      "Train Epoch: 22 [0/900 (0%)]\tLoss: 12.323605\n",
      "====> Epoch: 22 Average loss: 12.7173\n",
      "====> Test set loss: 12.7947\n",
      "Train Epoch: 23 [0/900 (0%)]\tLoss: 12.377480\n",
      "====> Epoch: 23 Average loss: 12.6941\n",
      "====> Test set loss: 12.7869\n",
      "Train Epoch: 24 [0/900 (0%)]\tLoss: 12.801034\n",
      "====> Epoch: 24 Average loss: 12.7251\n",
      "====> Test set loss: 12.6926\n",
      "Train Epoch: 25 [0/900 (0%)]\tLoss: 12.736019\n",
      "====> Epoch: 25 Average loss: 12.6681\n",
      "====> Test set loss: 12.8064\n",
      "Train Epoch: 26 [0/900 (0%)]\tLoss: 12.996169\n",
      "====> Epoch: 26 Average loss: 12.5994\n",
      "====> Test set loss: 12.6448\n",
      "Train Epoch: 27 [0/900 (0%)]\tLoss: 12.495996\n",
      "====> Epoch: 27 Average loss: 12.6447\n",
      "====> Test set loss: 12.9203\n",
      "Train Epoch: 28 [0/900 (0%)]\tLoss: 12.922414\n",
      "====> Epoch: 28 Average loss: 12.6007\n",
      "====> Test set loss: 12.7145\n",
      "Train Epoch: 29 [0/900 (0%)]\tLoss: 12.728524\n",
      "====> Epoch: 29 Average loss: 12.6154\n",
      "====> Test set loss: 12.6658\n",
      "Train Epoch: 30 [0/900 (0%)]\tLoss: 12.552384\n",
      "====> Epoch: 30 Average loss: 12.5538\n",
      "====> Test set loss: 12.7306\n",
      "[   1.    0.    1.   35.   20. 2560.    3.    3.    1.    1.    0.    0.\n",
      "    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "    0.    0.    0.    1.]\n",
      "[   1.    0.    1.   34.   22. 3460.    3.    3.    1.    1.    0.    0.\n",
      "    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "    0.    0.    0.    1.]\n",
      "[   1.    0.    1.   37.   15. 2454.    3.    3.    1.    1.    0.    0.\n",
      "    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "    0.    0.    0.    1.]\n",
      "[   1.    0.    1.   33.   21. 2278.    3.    3.    1.    1.    0.    0.\n",
      "    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "    0.    0.    0.    1.]\n",
      "[   1.    0.    1.   33.   21. 2757.    3.    3.    1.    1.    0.    0.\n",
      "    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "    0.    0.    0.    1.]\n"
     ]
    }
   ],
   "source": [
    "# no training\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/900 (0%)]\tLoss: 12.621139\n",
      "====> Epoch: 1 Average loss: 12.5358\n",
      "====> Test set loss: 12.7345\n",
      "Train Epoch: 2 [0/900 (0%)]\tLoss: 12.583764\n",
      "====> Epoch: 2 Average loss: 12.5463\n",
      "====> Test set loss: 12.6958\n",
      "Train Epoch: 3 [0/900 (0%)]\tLoss: 12.665889\n",
      "====> Epoch: 3 Average loss: 12.5365\n",
      "====> Test set loss: 12.8070\n",
      "Train Epoch: 4 [0/900 (0%)]\tLoss: 12.839505\n",
      "====> Epoch: 4 Average loss: 12.5669\n",
      "====> Test set loss: 12.6669\n",
      "Train Epoch: 5 [0/900 (0%)]\tLoss: 12.742674\n",
      "====> Epoch: 5 Average loss: 12.5547\n",
      "====> Test set loss: 12.7133\n",
      "Train Epoch: 6 [0/900 (0%)]\tLoss: 12.752720\n",
      "====> Epoch: 6 Average loss: 12.5201\n",
      "====> Test set loss: 12.6002\n",
      "Train Epoch: 7 [0/900 (0%)]\tLoss: 12.580607\n",
      "====> Epoch: 7 Average loss: 12.5358\n",
      "====> Test set loss: 12.5762\n",
      "Train Epoch: 8 [0/900 (0%)]\tLoss: 12.316352\n",
      "====> Epoch: 8 Average loss: 12.5052\n",
      "====> Test set loss: 12.5692\n",
      "Train Epoch: 9 [0/900 (0%)]\tLoss: 12.138209\n",
      "====> Epoch: 9 Average loss: 12.4717\n",
      "====> Test set loss: 12.4760\n",
      "Train Epoch: 10 [0/900 (0%)]\tLoss: 12.811017\n",
      "====> Epoch: 10 Average loss: 12.4802\n",
      "====> Test set loss: 12.5325\n",
      "Train Epoch: 11 [0/900 (0%)]\tLoss: 12.778244\n",
      "====> Epoch: 11 Average loss: 12.4591\n",
      "====> Test set loss: 12.5067\n",
      "Train Epoch: 12 [0/900 (0%)]\tLoss: 12.506435\n",
      "====> Epoch: 12 Average loss: 12.4988\n",
      "====> Test set loss: 12.6622\n",
      "Train Epoch: 13 [0/900 (0%)]\tLoss: 12.526202\n",
      "====> Epoch: 13 Average loss: 12.4249\n",
      "====> Test set loss: 12.6173\n",
      "Train Epoch: 14 [0/900 (0%)]\tLoss: 12.246674\n",
      "====> Epoch: 14 Average loss: 12.4345\n",
      "====> Test set loss: 12.6136\n",
      "Train Epoch: 15 [0/900 (0%)]\tLoss: 12.128992\n",
      "====> Epoch: 15 Average loss: 12.4269\n",
      "====> Test set loss: 12.4043\n",
      "Train Epoch: 16 [0/900 (0%)]\tLoss: 11.955967\n",
      "====> Epoch: 16 Average loss: 12.4339\n",
      "====> Test set loss: 12.5363\n",
      "Train Epoch: 17 [0/900 (0%)]\tLoss: 12.186446\n",
      "====> Epoch: 17 Average loss: 12.3752\n",
      "====> Test set loss: 12.4924\n",
      "Train Epoch: 18 [0/900 (0%)]\tLoss: 11.953749\n",
      "====> Epoch: 18 Average loss: 12.3907\n",
      "====> Test set loss: 12.4315\n",
      "Train Epoch: 19 [0/900 (0%)]\tLoss: 12.710867\n",
      "====> Epoch: 19 Average loss: 12.3447\n",
      "====> Test set loss: 12.4153\n",
      "Train Epoch: 20 [0/900 (0%)]\tLoss: 12.383488\n",
      "====> Epoch: 20 Average loss: 12.3390\n",
      "====> Test set loss: 12.3131\n",
      "Train Epoch: 21 [0/900 (0%)]\tLoss: 12.395462\n",
      "====> Epoch: 21 Average loss: 12.3524\n",
      "====> Test set loss: 12.4244\n",
      "Train Epoch: 22 [0/900 (0%)]\tLoss: 12.456567\n",
      "====> Epoch: 22 Average loss: 12.3812\n",
      "====> Test set loss: 12.5281\n",
      "Train Epoch: 23 [0/900 (0%)]\tLoss: 12.158492\n",
      "====> Epoch: 23 Average loss: 12.3323\n",
      "====> Test set loss: 12.4804\n",
      "Train Epoch: 24 [0/900 (0%)]\tLoss: 12.446365\n",
      "====> Epoch: 24 Average loss: 12.3160\n",
      "====> Test set loss: 12.4045\n",
      "Train Epoch: 25 [0/900 (0%)]\tLoss: 12.622350\n",
      "====> Epoch: 25 Average loss: 12.3487\n",
      "====> Test set loss: 12.4593\n",
      "Train Epoch: 26 [0/900 (0%)]\tLoss: 12.027983\n",
      "====> Epoch: 26 Average loss: 12.3612\n",
      "====> Test set loss: 12.4914\n",
      "Train Epoch: 27 [0/900 (0%)]\tLoss: 12.222124\n",
      "====> Epoch: 27 Average loss: 12.2961\n",
      "====> Test set loss: 12.1756\n",
      "Train Epoch: 28 [0/900 (0%)]\tLoss: 12.125057\n",
      "====> Epoch: 28 Average loss: 12.3473\n",
      "====> Test set loss: 12.4462\n",
      "Train Epoch: 29 [0/900 (0%)]\tLoss: 12.567684\n",
      "====> Epoch: 29 Average loss: 12.3137\n",
      "====> Test set loss: 12.3315\n",
      "Train Epoch: 30 [0/900 (0%)]\tLoss: 12.675349\n",
      "====> Epoch: 30 Average loss: 12.3407\n",
      "====> Test set loss: 12.2357\n",
      "[   0.    0.    0.   34.   21. 3389.    3.    3.    1.    1.    0.    0.\n",
      "    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "    0.    0.    0.    1.]\n",
      "[   1.    0.    1.   33.   21. 2224.    3.    3.    1.    1.    1.    0.\n",
      "    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "    0.    0.    0.    1.]\n",
      "[   1.    0.    1.   37.   23. 2935.    3.    3.    1.    1.    1.    0.\n",
      "    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "    0.    0.    0.    1.]\n",
      "[   0.    0.    0.   32.   21. 3480.    3.    3.    1.    1.    0.    0.\n",
      "    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "    0.    0.    0.    1.]\n",
      "[   1.    0.    0.   32.   18. 2236.    3.    3.    1.    1.    0.    0.\n",
      "    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "    0.    0.    0.    1.]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=28, out_features=60, bias=True)\n",
       "  (fc21): Linear(in_features=60, out_features=30, bias=True)\n",
       "  (fc22): Linear(in_features=60, out_features=30, bias=True)\n",
       "  (fc3): Linear(in_features=30, out_features=60, bias=True)\n",
       "  (fc4): Linear(in_features=60, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE().to(device)\n",
    "model.load_state_dict(torch.load(\"vae_lime_german.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-96c2cb5e9471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
